

	+-----------------------+
	|						|
	|	/--\				|
	|	|  |	RingFS		|
	|	\--/				|
	|						|
	+-----------------------+
	

I. DISTRIBUTED SYSTEMS

RingFS is a distributed filesystem offering scalability, fault-tolerance
and high-availability using replication.

1. Scalability means that new storage nodes can be added to the system
to increase storage and I/O throughput.

2. Fault-tolerance means the distributed system as a whole can tolerate
more faults than a single node, eg. the probability of data loss is less
than with a single node.

3. High-availability means that the probability of clients being able to
access and perform operations on the system is higher than with a single
node.

Distributed systems are not magic. If enough nodes fail, the system will
refuse to service write-requests and only service dirty-reads (loss of
availability). If all the disk drives of the nodes storing copies of a
file fail, the file is lost forever.

II. ABOUT THIS DOCUMENT

This document describes the design of RingFS. The flow of the document is
bottom-up, so we'll start at the basics and work our way up.

III. PAXOS CONSENSUS ALGORITHM

At the core of RingFS is Leslie Lamport's Paxos consensus algorithm. Paxos
is a distributed algorithm for nodes wishing to agree on a value, such as
a state transition.

NOTE:	Paxos makes no assumption about this 'value', it can be any seqeunce
		of bytes. In our implementation we use UDP to communicate this value,
		so its length is limited by the UDP datagram size.

If the software on the nodes implements a state machine, and all the nodes
agree on what the state transition should be, then the state machine will be
in the same states on all nodes. This is the 'distributed state machine'
approach to distributed systems. A better term is 'replicated state machine'.
Paxos is a majority algorithm, meaning that it is enough for a majority of
the nodes to be up to agree on a value. Paxos guarantees that if dead nodes
come back, they will eventually agree to the same value.

For more, see Lamport's 'Paxos made simple' paper. It is very hard to
understand the semantics of RingFS without a basic understanding of Paxos.

IV. REPLICATED LOG APPROACH

Paxos is a primitive used to reach consensus on a single value, which in our
case is a state transition command, or command for short. All nodes execute
this command on their local database. For a useful system, the nodes have to be
able to execute many commands in sequence, and agree on the ordering of the
commands. This is the replicated log approach to distributed systems.

In the replicated log approach, the nodes accept client connections, take
database commands from the clients, and attempt to get the other nodes to
accept this command as the next value. If another node's command "wins out",
the node tries again in the next round of Paxos. The only consistency
guarantee required is that all nodes agree on the order of the commands,
but the order itself is irrelevant.

For more, see the Google 'Paxos made live' paper. It is very hard to
understand the semantics of RingFS without a basic understanding of the
replicated log approach.

V. HIGH-LEVEL OVERVIEW

At a high-level, the Ring filesystem consists of

-	'paxos disks' responsible for storing file contents,
-	a 'controller' responsible for overall configuration of the system
	and file metadata,
-	clients running the RingFS client library.

Paxos disks and the controller are a logical entity; physically, they are
a replicated distributed system made up of several nodes.

VI. PAXOS DISKS

Paxos disks are the object-store abstraction in RingFS. Clients read and write
file contents and read file length from paxos disks. A paxos disk offers a
simple key -> value storage, where a key is a large integer, like an inode,
identifying the file, and the value is the contents of the file.

NOTE:	In RingFS, the length of files are stored together with their contents.
		If the files in a directory are stored on different paxos disks, then
		if the user issues an 'ls -l' command, the client library will
		fetch the list of files and their locations from the controller,
		and the lengths of files from the appropriate nodes. In other words,
		'ls' is cheap, changing the length of a file is cheap, but 'ls -l'
		is expensive.

More specifically, clients can choose between safe-reads and dirty-reads. Safe
means that all previous write operations are reflected in the read value. No
such guarantee is given for dirty-reads. Dirty-reads are not necessairily 
faster, but even a single live node can service it.

Physically, paxos disks are n-way replicated and use the replicated log
approach to agree on which node stores which version of a file. The nodes
do not use a replicated log or paxos to exchange the contents of files;
this is done in the 'background' through TCP connections.

NOTE:	The version is a monotonically increasing integer for each file.

The paxos disk maintains a per file state machine: for each file, the paxos
disk nodes maintain a state machine. Suppose the cell consists of n = 3
nodes, then the state is: (s1, s2, s3) where si is a version number of the
file or 'do not have it'. Suppose the state is (0, 0, 0), ie. all nodes
store file contents version 0. A client connects to node 1 and uploads a
new file version. Upon completion, the node performs a round of paxos to
set the state to (1, 0, 0). At this point the majority of the nodes is
aware of the fact that the latest version of this file is 1 and everybody
can check their own version to figure out if they need to copy the file
over from node 1. After node 2 copies the file from node 1, it performs
a round of paxos to change the state to (1, 1, 0), and so on. Notice that
in paxos a majority consensus is required, but this doesn't mean that a
majority of the nodes have to have a copy of the latest version --- they
just know about the latest version. The file versions are stored in a
persistent manner in the VersionDB. On each update, the VersionDB is
written. The ReplicatedLog is in-memory only, used for helping lagging
nodes catch up.

The architecture of RingFS is optimized to offer cheap reads. The client
contacts a member of the paxos group and issues a safe-read command. Without
further assumptions, the node would have to insert the read command into
the replicated log to be sure it has received all previous write commands,
since the rest of the group, forming a majority, could have made progress
without it. In order to avoid a round of paxos for each read, paxos disks
use a master-slave model on top of paxos.

The group uses a paxos round to elect a master. The controller is informed
who the current master is, and directs clients' read and write requests to
the master. The rest of the nodes are slaves. Slaves refuse to service
write requests as long as they believe the master is alive. This optimization
allows the master to service safe-read requests without performing a round
of paxos, since all write operatios go through it.

The paxos group has a designated node, which if it is alive and can reach
a majority of the group will always try to re-claim mastership. This is an
IO optimization related to the paxos ring architecture, see later.

In order to guarantee correct master-slave semantics paxos disks use
master-leases. The master reaffirms its mastership every 1 second through
paxos. If it is unable to extends its lease for 3 seconds, it assumes it
has lost its mastership. The other nodes, after not extending the master
lease for 5 seconds, will start volunteering to become the next master.
This is called master-failover.

Note that slave nodes can still service safe-read requests, but they have to
perform a round of paxos to be sure their replicated log reflects the latest
state of the paxos disk. This master-slave model does not mean that only the
master issues paxos prepare and propose messages, so we do not employ the
multi-paxos algorithm optimization.

Writing a file involves receiving the file contents from the client, performing
a round of paxos to inform the other nodes of the new version and finally
telling the client that its operation succeeded. One must take into account
that the node servicing the request can fail at any time during this process.

When the client sends the file contents to the node, the old file contents are
copied over to a 'backup area' --- but only the part of the file that is being
overwritten. After the client finished sending over the contents of the file,
the node performs a round of paxos to tell the other nodes about the new version.
After the paxos round succeeded the node deletes the backup file.

If the node dies while receiving the contents of the file, the file in the
'live area' will be corrupted. It will have to revert: overwrite the relevant
parts of the file with the backup file.

If the node dies after performing the round of paxos but before removing the
backup file, it will still revert to the backup file. This is not a problem,
this just means that a 'dummy' version increase happened, where the new version
is the same as the old version. There was no data loss. Since the client did
not get the OK message, it will subsequently try again.

If the node dies after it has removed the backup but before sending the OK
to the client, then the operation has commited but the client did not get
notified. The client will subsequently try again, and write the same version
again. (This cannot be handled in any case.)

When writing data to a paxos disk, the client can choose between a single-write
and a majority-write. A single-write consists of

1.	the client initiates a write operation at the master
2.	the client sends the contents of the file to the master, which
	stores the old version in the 'backup area'
3.	after the client finished writing, the master performs a round of paxos
	notifying the other nodes that a new file version has entered the paxos
	disk (for a round of paxos to succeed a majority of the nodes have to be
	alive)
4.	the master removes the backup,
5.	the master then acknowledges the operation to the client
*.	subsequently, the rest of the nodes will copy over the contents of the
	file from the master and perform rounds of paxos telling the other
	nodes they have this latest version.

NOTE:	Version 1.0 of RingFS supports single-writes only. Majority-writes
		are useful if the client wants to make sure that the file has been
		stored in a persistent manner on more than one node. This is important,
		because if the node servicing a write request dies before any other
		nodes can copy over the new version, than that write is lost forever.

When servicing a safe-read request, the node first checks if it is the master:

1. If it is the master, it does not have to run a round of paxos for the
read request itself. It then checks its file version database (VersionDB)
what the latest version of this file in the paxos disk is. Note that the
master may not have this version on its local disk, which is reflected in
VersionDB. If it does have the latest version, it sends the file contents
to the client. If it does not have the latest version, it sends a list of
nodes that do to the client (redirect). These nodes may not be up, in which
case a safe read is not possible.

2. If the node is not the master, it examines its VersionDB to see whether it
has the latest version on its local disk. If it does, it runs a round of paxos
to serialize the read operaion. If it succeeds in serializing the read operation
before any other writes to this file, it returns the file contents to the client.
If it does not have the latest version, it sends a list of nodes that do to the
client (redirect).  

NOTE:	The client will give up after five redirects.

When servicing a dirty-read request, the node never runs paxos. It checks
whether is has any copy of the file, and returns it to the client. If it
does not have a copy of the file, it redirects the clients to all other nodes.

Requesting the length of a file is logically equivalent to requesting the
contents of the file and counting the bytes on the client side. The operation
- like reads - can proceed in a safe or dirty fashion. A safe-read can only be
serviced by a node if it locally stores the latest version of a file. If the
node servicing the request is not the master, it additionally has to run a
round of paxos to make sure the other nodes have not made progress on this
file without it. Dirty reads can be served by any node (if a node does not
have a local copy of a file, it returns 0 as the length).

NOTE:	RingFS does not guarantee that in the time between a client requesting
		the length of a file (e.g. 'ls -l') and fetching the contents (e.g. 'cat')
		the file length is not changed by another client, even when both
		are safe-reads. The same thing can happen on a local disk. To get
		this kind of guarantee, the client must obtain a lock from a distributed
		lock manager.

Clients can also request a file to be deleted. Guaranteeing correct semantics
is simple: the node receiving the command performs a round of paxos, and all
nodes delete the VersionDB entry and delete the file. The only tricky
part is dealing with lagging nodes. Lagging nodes do not participate in paxos
rounds for a while, and then quickly fetch the missing paxos values. If a node
was out for only a short while, it will simply fetch the missing ReplicatedLog
entries from the other nodes and learn that the file was deleted. If it was
out for a longer time, it will fetch another node's entire VersionDB. If a
file was deleted, this fetched VersionDB will be missing all information about
the file. The node thus has to match the fetched VersionDB and its local
'live area' and delete any stale files. This is not a real complication, since
nodes have to do this anyway: if a node goes down after deleting the VersionDB
entry but before deleting the file from its local disk, it has to somehow
reclaim the space later. Thus, nodes have to periodically run a garbage
collector in the background which detects and deletes stale files in the 
'live area'.

Nodes have to handle overlapping file operations. If a client is performing a
read operation, and another is trying to [over]write the same file, the node
has to block the writer until the reader is finished. This is accomplised by
having in-memory per-file reader-writer locks. In a single-threaded eventloop
this can be a simple hashmap.

RingFS offers fault-tolerance through replication by attempting to detect
local disk corruption. For each file, it records a 32-bit CRC, and
periodically re-checks the file. If it finds that the file is corrupted, it
informs the other nodes that it lost its copy of the file by performing a
round of paxos and setting the value for its version to 'do not have it'.
The corrupted file is deleted from disk.

The operation of a paxos disk node, upon startup:
-	read the paxos disk configuration from local disk
-	revert files in the backup area
-	read the VersionDB from local disk into memory
-	read the paxos state of the last active round:
	o	paxosID
	o	highest n promised and
	o	the last accepted value, if any
-	start the eventloop

Local configuration:
-	list of nodes making up the paxos disk
-	who is the designated master

Local timers:
-	perform cyclic redundancy check
-	find files where other nodes have newer versions and copy them over
-	find stale files in the 'live area'

Paxos group events:
-	designated master: if it is not currently the master, try to become the master
-	master: every 1 second, extend the master lease
-	master: if unable to extend the master lease for 3 seconds, give up mastership
-	slave: every 5 seconds, check if the master is alive; if not, attempt to become the
	master

VII. PAXOS RING ARCHITECTURE

*** I need to write this up. ***

VIII. CONTROLLER

*** I need to write this up. ***

IX. CLIENTS

Later...

X. DISTRIBUTED CACHING

Later...

XI. DISTRIBUTED LOCKING

Later...
